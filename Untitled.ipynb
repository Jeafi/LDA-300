{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\py36\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import os\n",
    "# # 重新编号\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    i=0\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        k = len(str(i))\n",
    "        j = ''\n",
    "        for co in range(0,8-k):\n",
    "            j = j + '0'\n",
    "        j = j + str(i) + '.txt'\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        os.rename(tmp_path, os.path.join(tmp_pathall,j))\n",
    "        i = i + 1\n",
    "# 收集训练集\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    if len(fs) <= 10:\n",
    "        continue\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        ff = open(tmp_path, encoding='utf-8')\n",
    "        lines = ff.readlines()\n",
    "        text = []\n",
    "        # 将文件中的词装入列表\n",
    "        for line in lines:\n",
    "            if len(line) != 1:\n",
    "                # 抽掉引用法条\n",
    "                if line.split('##')[0] != '引用法条' and line.split('##')[0] != '特征' and line.split('##')[0] != '案由':\n",
    "                    for word in line.split('##')[1].split('$'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "                if line.split('##')[0] == '特征':\n",
    "                    for word in line.split('##')[1].split('\\t'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "                if line.split('##')[0] == '案由':\n",
    "                    for word in line.split('##')[1].split('\\t'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "        ff.close()\n",
    "        train_set.append(text)\n",
    "    output = os.path.join(fr, f1) #输出文件目录\n",
    "    if not os.path.exists(output):    \n",
    "        os.makedirs(output)\n",
    "                # 生成字典\n",
    "    dictionary = corpora.Dictionary(train_set)\n",
    "            # 去除极低频的杂质词\n",
    "    \n",
    "    dictionary.filter_extremes(no_below=1, no_above=1, keep_n=None)\n",
    "            # 将词典保存下来，方便后续使用\n",
    "    \n",
    "    dictionary.save(os.path.join(output, \"all.dic\"))\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in train_set]\n",
    "            # 使用数字语料生成TFIDF模型\n",
    "    \n",
    "    tfidfModel = models.TfidfModel(corpus)\n",
    "            # 存储tfidfModel\n",
    "    \n",
    "    tfidfModel.save(os.path.join(output, \"allTFIDF.mdl\"))\n",
    "            # 把全部语料向量化成TFIDF模式，这个tfidfModel可以传入二维数组   \n",
    "    \n",
    "    tfidfVectors = tfidfModel[corpus]\n",
    "            # 建立索引并保存\n",
    "    \n",
    "    indexTfidf = similarities.MatrixSimilarity(tfidfVectors)\n",
    "    \n",
    "    indexTfidf.save(os.path.join(output,  \"allTFIDF.idx\"))\n",
    "            # 通过TFIDF向量生成LDA模型，id2word表示编号的对应词典，num_topics表示主题数，我们这里设定的50，主题太多时间受不了。\n",
    "    \n",
    "    lda = models.LdaModel(tfidfVectors, id2word=dictionary, num_topics=100)\n",
    "            # 把模型保存下来\n",
    "    \n",
    "    lda.save(os.path.join(output, \"allLDA50Topic.mdl\"))\n",
    "            # 把所有TFIDF向量变成LDA的向量\n",
    "    \n",
    "    corpus_lda = lda[tfidfVectors]\n",
    "            # 建立索引，把LDA数据保存下来\n",
    "    \n",
    "    indexLDA = similarities.MatrixSimilarity(corpus_lda)\n",
    "    \n",
    "    indexLDA.save(os.path.join(output,\"allLDA50Topic.idx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[412, 75, 873, 789, 198, 413, 470, 65, 379, 675]\n",
      "1363210\n",
      "\n",
      "1606411\n",
      "\n",
      "763568\n",
      "\n",
      "561563\n",
      "\n",
      "1847264\n",
      "\n",
      "2269433\n",
      "\n",
      "2399045\n",
      "\n",
      "1071271\n",
      "\n",
      "1354667\n",
      "\n",
      "2882162\n",
      "\n",
      "[412, 49, 136, 168, 813, 629, 318, 28, 660, 212]\n",
      "1363210\n",
      "\n",
      "1554522\n",
      "\n",
      "1709003\n",
      "\n",
      "1319219\n",
      "\n",
      "626144\n",
      "\n",
      "2744406\n",
      "\n",
      "2084990\n",
      "\n",
      "1512262\n",
      "\n",
      "2843550\n",
      "\n",
      "1325648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wenshu='00000412.txt'\n",
    "anyou = '帮助毁灭、伪造证据罪'\n",
    "fs = os.path.join(r'alltext',anyou)\n",
    "fr = r'result' \n",
    "f = open(os.path.join(fs,wenshu),encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "for line in lines:#将文件中的词装入列表\n",
    "     if len(line) != 1:\n",
    "            # 抽掉引用法条\n",
    "            if line.split('##')[0] != '引用法条' and line.split('##')[0] != '特征' and line.split('##')[0] != '案由':\n",
    "                for word in line.split('##')[1].split('$'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "            if line.split('##')[0] == '特征':\n",
    "                for word in line.split('##')[1].split('\\t'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "            if line.split('##')[0] == '案由':\n",
    "                for word in line.split('##')[1].split('\\t'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "#到该案由路径下载入获取已经训练好的模型\n",
    "\n",
    "output = os.path.join (fr,anyou)\n",
    "# 载入字典\n",
    "dictionary = corpora.Dictionary.load(os.path.join(output,\"all.dic\"))\n",
    "# 载入TFIDF模型和索引\n",
    "tfidfModel = models.TfidfModel.load(os.path.join(output,\"allTFIDF.mdl\"))\n",
    "indexTfidf = similarities.MatrixSimilarity.load(os.path.join(output,\"allTFIDF.idx\"))\n",
    "# # 载入LDA模型和索引\n",
    "ldaModel = models.LdaModel.load(os.path.join(output,  \"allLDA50Topic.mdl\"))\n",
    "indexLDA = similarities.MatrixSimilarity.load(os.path.join (output,\"allLDA50Topic.idx\"))\n",
    "#query就是测试数据，先切词\n",
    "query_bow = dictionary.doc2bow(text)\n",
    "#使用TFIDF模型向量化\n",
    "tfidfvect = tfidfModel[query_bow]\n",
    "#然后LDA向量化，因为我们训练时的LDA是在TFIDF基础上做的，所以用itidfvect再向量化一次\n",
    "ldavec = ldaModel[tfidfvect]\n",
    "#TFIDF相似性，相似性列表记录与每份模型中的文书的相似度\n",
    "simstfidf = indexTfidf[tfidfvect]\n",
    "#LDA相似性，相似性列表记录与每份模型中的文书的相似度\n",
    "simlda = indexLDA[ldavec]\n",
    "# 获取TDIDF模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "num = []\n",
    "# 记录相似度\n",
    "gets = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simstfidf:\n",
    "        if m < likely and likely not in gets:\n",
    "                m = likely\n",
    "                marknow = mark\n",
    "        mark = mark+1\n",
    "    gets.append(m)\n",
    "    num.append(marknow)\n",
    "print(num)\n",
    "\n",
    "bianhao = []\n",
    "for fn in num:\n",
    "    k = len(str(fn))\n",
    "    j = ''\n",
    "    for co in range(0,8-k):\n",
    "        j = j + '0'\n",
    "    j = j + str(fn) + '.txt'\n",
    "    fl = open(os.path.join(fs,j),encoding = 'UTF-8')\n",
    "    lines = fl.readlines()\n",
    "    for line in lines:#将文件中的编号装入列表\n",
    "        if len(line) != 1:\n",
    "            if line.split('##')[0] == '编号':\n",
    "                print(line.split('##')[1])\n",
    "                bianhao.append(line.split('##')[1])\n",
    "    fl.close()\n",
    "# print(bianhao)\n",
    "\n",
    "# 获取LDA模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "numlda = []\n",
    "# 记录相似度\n",
    "getslda = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simlda:\n",
    "        if m < likely and likely not in getslda:\n",
    "            m = likely\n",
    "            marknow = mark\n",
    "        mark = mark+1\n",
    "    getslda.append(m)\n",
    "    numlda.append(marknow)\n",
    "print(numlda)\n",
    "\n",
    "bianhaolda = []\n",
    "for fn in numlda:\n",
    "    k = len(str(fn))\n",
    "    j = ''\n",
    "    for co in range(0,8-k):\n",
    "        j = j + '0'\n",
    "    j = j + str(fn) + '.txt'\n",
    "    fl = open(os.path.join(fs,j),encoding = 'UTF-8')\n",
    "    lines = fl.readlines()\n",
    "    for line in lines:#将文件中的编号装入列表\n",
    "        if len(line) != 1:\n",
    "            if line.split('##')[0] == '编号':\n",
    "                print(line.split('##')[1])\n",
    "                bianhaolda.append(line.split('##')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99999565,\n",
       " 0.999,\n",
       " 0.99852777,\n",
       " 0.9812098,\n",
       " 0.97203577,\n",
       " 0.9558123,\n",
       " 0.9223232,\n",
       " 0.8299504,\n",
       " 0.7342617,\n",
       " 0.6620344]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getslda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.33775526,\n",
       " 0.28847206,\n",
       " 0.16199163,\n",
       " 0.14654307,\n",
       " 0.12300442,\n",
       " 0.107481554,\n",
       " 0.10245913,\n",
       " 0.097217605,\n",
       " 0.09541534]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = os.listdir(r'alltext')\n",
    "len(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10578, 4908, 16473, 1272, 1044, 7996, 3780, 19378, 7995]\n",
      "[0, 136, 8270, 3377, 4739, 4738, 4776, 3543, 1449, 4733]\n"
     ]
    }
   ],
   "source": [
    "# 获取TDIDF模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "num = []\n",
    "# 记录相似度\n",
    "gets = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simstfidf:\n",
    "        if m < likely and likely not in gets:\n",
    "                m = likely\n",
    "                marknow = mark\n",
    "        mark = mark+1\n",
    "    gets.append(m)\n",
    "    num.append(marknow)\n",
    "print(num)\n",
    "\n",
    "# 获取LDA模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "numlda = []\n",
    "# 记录相似度\n",
    "getslda = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simlda:\n",
    "        if m < likely and likely not in getslda:\n",
    "            m = likely\n",
    "            marknow = mark\n",
    "        mark = mark+1\n",
    "    getslda.append(m)\n",
    "    numlda.append(marknow)\n",
    "print(numlda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.01392374, 0.01199614, ..., 0.03279732, 0.0635283 ,\n",
       "       0.04038719], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simstfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帮助犯罪分子逃避处罚罪抢劫罪受贿罪\n"
     ]
    }
   ],
   "source": [
    "fr = r'result'\n",
    "print (anyou)\n",
    "output = os.path.join(fr, anyou) #输出文件目录\n",
    "if not os.path.exists(output):    \n",
    "    os.makedirs(output)\n",
    "        # 生成字典\n",
    "dictionary = corpora.Dictionary(train_set)\n",
    "    # 去除极低频的杂质词\n",
    "dictionary.filter_extremes(no_below=1, no_above=1, keep_n=None)\n",
    "    # 将词典保存下来，方便后续使用\n",
    "dictionary.save(os.path.join(output, \"all.dic\"))\n",
    "corpus = [dictionary.doc2bow(text) for text in train_set]\n",
    "    # 使用数字语料生成TFIDF模型\n",
    "tfidfModel = models.TfidfModel(corpus)\n",
    "    # 存储tfidfModel\n",
    "tfidfModel.save(os.path.join(output, \"allTFIDF.mdl\"))\n",
    "    # 把全部语料向量化成TFIDF模式，这个tfidfModel可以传入二维数组\n",
    "    \n",
    "tfidfVectors = tfidfModel[corpus]\n",
    "    # 建立索引并保存\n",
    "indexTfidf = similarities.MatrixSimilarity(tfidfVectors)\n",
    "indexTfidf.save(os.path.join(output,  \"allTFIDF.idx\"))\n",
    "    # 通过TFIDF向量生成LDA模型，id2word表示编号的对应词典，num_topics表示主题数，我们这里设定的50，主题太多时间受不了。\n",
    "lda = models.LdaModel(tfidfVectors, id2word=dictionary, num_topics=100)\n",
    "    # 把模型保存下来\n",
    "lda.save(os.path.join(output, \"allLDA50Topic.mdl\"))\n",
    "    # 把所有TFIDF向量变成LDA的向量\n",
    "corpus_lda = lda[tfidfVectors]\n",
    "    # 建立索引，把LDA数据保存下来\n",
    "indexLDA = similarities.MatrixSimilarity(corpus_lda)\n",
    "indexLDA.save(os.path.join(output,\"allLDA50Topic.idx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 重新编号\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    i=0\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        k = len(str(i))\n",
    "        j = ''\n",
    "        for co in range(0,8-k):\n",
    "            j = j + '0'\n",
    "        j = j + str(i) + '.txt'\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        os.rename(tmp_path, os.path.join(tmp_pathall,j))\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
