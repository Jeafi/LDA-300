{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import os\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        ff = open(tmp_path, encoding='utf-8')\n",
    "        lines = ff.readlines()\n",
    "        text = []\n",
    "        # 将文件中的词装入列表\n",
    "        for line in lines:\n",
    "            if len(line) != 1:\n",
    "                # 抽掉引用法条\n",
    "                if line.split('##')[0] != '引用法条' and line.split('##')[0] != '特征' and line.split('##')[0] != '案由':\n",
    "                    for word in line.split('##')[1].split('$'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "                if line.split('##')[0] == '特征':\n",
    "                    for word in line.split('##')[1].split('\\t'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "                if line.split('##')[0] == '案由':\n",
    "                    for word in line.split('##')[1].split('\\t'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "        train_set.append(text)\n",
    "    output = os.path.join(fr, f1) #输出文件目录\n",
    "    if not os.path.exists(output):    \n",
    "        os.makedirs(output)\n",
    "                # 生成字典\n",
    "    dictionary = corpora.Dictionary(train_set)\n",
    "            # 去除极低频的杂质词\n",
    "    \n",
    "    dictionary.filter_extremes(no_below=1, no_above=1, keep_n=None)\n",
    "            # 将词典保存下来，方便后续使用\n",
    "    \n",
    "    dictionary.save(os.path.join(output, \"all.dic\"))\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in train_set]\n",
    "            # 使用数字语料生成TFIDF模型\n",
    "    \n",
    "    tfidfModel = models.TfidfModel(corpus)\n",
    "            # 存储tfidfModel\n",
    "    \n",
    "    tfidfModel.save(os.path.join(output, \"allTFIDF.mdl\"))\n",
    "            # 把全部语料向量化成TFIDF模式，这个tfidfModel可以传入二维数组   \n",
    "    \n",
    "    tfidfVectors = tfidfModel[corpus]\n",
    "            # 建立索引并保存\n",
    "    \n",
    "    indexTfidf = similarities.MatrixSimilarity(tfidfVectors)\n",
    "    \n",
    "    indexTfidf.save(os.path.join(output,  \"allTFIDF.idx\"))\n",
    "            # 通过TFIDF向量生成LDA模型，id2word表示编号的对应词典，num_topics表示主题数，我们这里设定的50，主题太多时间受不了。\n",
    "    \n",
    "    lda = models.LdaModel(tfidfVectors, id2word=dictionary, num_topics=100)\n",
    "            # 把模型保存下来\n",
    "    \n",
    "    lda.save(os.path.join(output, \"allLDA50Topic.mdl\"))\n",
    "            # 把所有TFIDF向量变成LDA的向量\n",
    "    \n",
    "    corpus_lda = lda[tfidfVectors]\n",
    "            # 建立索引，把LDA数据保存下来\n",
    "    \n",
    "    indexLDA = similarities.MatrixSimilarity(corpus_lda)\n",
    "    \n",
    "    indexLDA.save(os.path.join(output,\"allLDA50Topic.idx\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308, 308, 308, 308, 308, 308, 308, 308, 308, 308]\n",
      "[308, 308, 308, 308, 308, 308, 308, 308, 308, 308]\n"
     ]
    }
   ],
   "source": [
    "f = open(r'alltext\\帮助犯罪分子逃避处罚罪\\8251.txt',encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "anyou = ''\n",
    "for line in lines:#将文件中的词装入列表\n",
    "     if len(line) != 1:\n",
    "            # 抽掉引用法条\n",
    "            if line.split('##')[0] != '引用法条' and line.split('##')[0] != '特征' and line.split('##')[0] != '案由':\n",
    "                for word in line.split('##')[1].split('$'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "            if line.split('##')[0] == '特征':\n",
    "                for word in line.split('##')[1].split('\\t'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "            if line.split('##')[0] == '案由':\n",
    "                for word in line.split('##')[1].split('\\t'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "#到该案由路径下载入获取已经训练好的模型\n",
    "fr = r'result'\n",
    "output = os.path.join (fr,'帮助犯罪分子逃避处罚罪')\n",
    "# 载入字典\n",
    "dictionary = corpora.Dictionary.load(os.path.join(output,\"all.dic\"))\n",
    "# 载入TFIDF模型和索引\n",
    "tfidfModel = models.TfidfModel.load(os.path.join(output,\"allTFIDF.mdl\"))\n",
    "indexTfidf = similarities.MatrixSimilarity.load(os.path.join(output,\"allTFIDF.idx\"))\n",
    "# 载入LDA模型和索引\n",
    "ldaModel = models.LdaModel.load(os.path.join(output,  \"allLDA50Topic.mdl\"))\n",
    "indexLDA = similarities.MatrixSimilarity.load(os.path.join (output,\"allLDA50Topic.idx\"))\n",
    "#query就是测试数据，先切词\n",
    "query_bow = dictionary.doc2bow(text)\n",
    "#使用TFIDF模型向量化\n",
    "tfidfvect = tfidfModel[query_bow]\n",
    "#然后LDA向量化，因为我们训练时的LDA是在TFIDF基础上做的，所以用itidfvect再向量化一次\n",
    "ldavec = ldaModel[tfidfvect]\n",
    "#TFIDF相似性，相似性列表记录与每份模型中的文书的相似度\n",
    "simstfidf = indexTfidf[tfidfvect]\n",
    "#LDA相似性，相似性列表记录与每份模型中的文书的相似度\n",
    "simlda = indexLDA[ldavec]\n",
    "\n",
    "# 获取TDIDF模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "num = []\n",
    "# 记录相似度\n",
    "gets = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 1\n",
    "    for likely in simstfidf:\n",
    "        if m < likely: \n",
    "            if not m in gets:\n",
    "                m = likely\n",
    "                marknow = mark\n",
    "        mark = mark+1\n",
    "    gets.append(m)\n",
    "    num.append(marknow)\n",
    "print(num)\n",
    "\n",
    "# 获取LDA模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "numlda = []\n",
    "# 记录相似度\n",
    "getslda = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 1\n",
    "    for likely in simlda:\n",
    "        if m < likely and m not in gets:\n",
    "            m = likely\n",
    "            marknow = mark\n",
    "        mark = mark+1\n",
    "    getslda.append(m)\n",
    "    numlda.append(marknow)\n",
    "print(numlda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帮助犯罪分子逃避处罚罪抢劫罪受贿罪\n"
     ]
    }
   ],
   "source": [
    "fr = r'result'\n",
    "print (anyou)\n",
    "output = os.path.join(fr, anyou) #输出文件目录\n",
    "if not os.path.exists(output):    \n",
    "    os.makedirs(output)\n",
    "        # 生成字典\n",
    "dictionary = corpora.Dictionary(train_set)\n",
    "    # 去除极低频的杂质词\n",
    "dictionary.filter_extremes(no_below=1, no_above=1, keep_n=None)\n",
    "    # 将词典保存下来，方便后续使用\n",
    "dictionary.save(os.path.join(output, \"all.dic\"))\n",
    "corpus = [dictionary.doc2bow(text) for text in train_set]\n",
    "    # 使用数字语料生成TFIDF模型\n",
    "tfidfModel = models.TfidfModel(corpus)\n",
    "    # 存储tfidfModel\n",
    "tfidfModel.save(os.path.join(output, \"allTFIDF.mdl\"))\n",
    "    # 把全部语料向量化成TFIDF模式，这个tfidfModel可以传入二维数组\n",
    "    \n",
    "tfidfVectors = tfidfModel[corpus]\n",
    "    # 建立索引并保存\n",
    "indexTfidf = similarities.MatrixSimilarity(tfidfVectors)\n",
    "indexTfidf.save(os.path.join(output,  \"allTFIDF.idx\"))\n",
    "    # 通过TFIDF向量生成LDA模型，id2word表示编号的对应词典，num_topics表示主题数，我们这里设定的50，主题太多时间受不了。\n",
    "lda = models.LdaModel(tfidfVectors, id2word=dictionary, num_topics=100)\n",
    "    # 把模型保存下来\n",
    "lda.save(os.path.join(output, \"allLDA50Topic.mdl\"))\n",
    "    # 把所有TFIDF向量变成LDA的向量\n",
    "corpus_lda = lda[tfidfVectors]\n",
    "    # 建立索引，把LDA数据保存下来\n",
    "indexLDA = similarities.MatrixSimilarity(corpus_lda)\n",
    "indexLDA.save(os.path.join(output,\"allLDA50Topic.idx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    i=0\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        j = '00'+str(i)+'.txt'\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        os.rename(tmp_path, os.path.join(tmp_pathall,j))\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
