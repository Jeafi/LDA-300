{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import os\n",
    "# 重新编号\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    i=0\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        j = '00'+str(i)+'.txt'\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        os.rename(tmp_path, os.path.join(tmp_pathall,j))\n",
    "        i = i+1\n",
    "# 收集训练集\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    if len(fs) <= 10:\n",
    "        continue\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        ff = open(tmp_path, encoding='utf-8')\n",
    "        lines = ff.readlines()\n",
    "        text = []\n",
    "        # 将文件中的词装入列表\n",
    "        for line in lines:\n",
    "            if len(line) != 1:\n",
    "                # 抽掉引用法条\n",
    "                if line.split('##')[0] != '引用法条' and line.split('##')[0] != '特征' and line.split('##')[0] != '案由':\n",
    "                    for word in line.split('##')[1].split('$'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "                if line.split('##')[0] == '特征':\n",
    "                    for word in line.split('##')[1].split('\\t'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "                if line.split('##')[0] == '案由':\n",
    "                    for word in line.split('##')[1].split('\\t'):\n",
    "                        text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "        ff.close()\n",
    "        train_set.append(text)\n",
    "    output = os.path.join(fr, f1) #输出文件目录\n",
    "    if not os.path.exists(output):    \n",
    "        os.makedirs(output)\n",
    "                # 生成字典\n",
    "    dictionary = corpora.Dictionary(train_set)\n",
    "            # 去除极低频的杂质词\n",
    "    \n",
    "    dictionary.filter_extremes(no_below=1, no_above=1, keep_n=None)\n",
    "            # 将词典保存下来，方便后续使用\n",
    "    \n",
    "    dictionary.save(os.path.join(output, \"all.dic\"))\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in train_set]\n",
    "            # 使用数字语料生成TFIDF模型\n",
    "    \n",
    "    tfidfModel = models.TfidfModel(corpus)\n",
    "            # 存储tfidfModel\n",
    "    \n",
    "    tfidfModel.save(os.path.join(output, \"allTFIDF.mdl\"))\n",
    "            # 把全部语料向量化成TFIDF模式，这个tfidfModel可以传入二维数组   \n",
    "    \n",
    "    tfidfVectors = tfidfModel[corpus]\n",
    "            # 建立索引并保存\n",
    "    \n",
    "    indexTfidf = similarities.MatrixSimilarity(tfidfVectors)\n",
    "    \n",
    "    indexTfidf.save(os.path.join(output,  \"allTFIDF.idx\"))\n",
    "#             # 通过TFIDF向量生成LDA模型，id2word表示编号的对应词典，num_topics表示主题数，我们这里设定的50，主题太多时间受不了。\n",
    "    \n",
    "#     lda = models.LdaModel(tfidfVectors, id2word=dictionary, num_topics=100)\n",
    "#             # 把模型保存下来\n",
    "    \n",
    "#     lda.save(os.path.join(output, \"allLDA50Topic.mdl\"))\n",
    "#             # 把所有TFIDF向量变成LDA的向量\n",
    "    \n",
    "#     corpus_lda = lda[tfidfVectors]\n",
    "#             # 建立索引，把LDA数据保存下来\n",
    "    \n",
    "#     indexLDA = similarities.MatrixSimilarity(corpus_lda)\n",
    "    \n",
    "#     indexLDA.save(os.path.join(output,\"allLDA50Topic.idx\"))\n",
    "#     os.remove(os.path.join(output,\"allLDA50Topic.mdl.expElogbeta.npy\"))\n",
    "#     os.remove(os.path.join(output,\"allLDA50Topic.mdl.id2word\"))\n",
    "#     os.remove(os.path.join(output,\"allLDA50Topic.mdl.state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 669, 102, 13, 352, 770, 284, 331, 678, 645]\n",
      "1005796\n",
      "\n",
      "2666238\n",
      "\n",
      "1243923\n",
      "\n",
      "1056149\n",
      "\n",
      "1861515\n",
      "\n",
      "395030\n",
      "\n",
      "1694014\n",
      "\n",
      "1788889\n",
      "\n",
      "2680845\n",
      "\n",
      "2610613\n",
      "\n",
      "['1005796\\n', '2666238\\n', '1243923\\n', '1056149\\n', '1861515\\n', '395030\\n', '1694014\\n', '1788889\\n', '2680845\\n', '2610613\\n']\n"
     ]
    }
   ],
   "source": [
    "wenshu='000.txt'\n",
    "anyou = '帮助毁灭、伪造证据罪'\n",
    "fs = os.path.join(r'alltext',anyou)\n",
    "fr = r'result' \n",
    "f = open(os.path.join(fs,wenshu),encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "for line in lines:#将文件中的词装入列表\n",
    "     if len(line) != 1:\n",
    "            # 抽掉引用法条\n",
    "            if line.split('##')[0] != '引用法条' and line.split('##')[0] != '特征' and line.split('##')[0] != '案由':\n",
    "                for word in line.split('##')[1].split('$'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "            if line.split('##')[0] == '特征':\n",
    "                for word in line.split('##')[1].split('\\t'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "            if line.split('##')[0] == '案由':\n",
    "                for word in line.split('##')[1].split('\\t'):\n",
    "                    text.append(word.replace('\\n', '').replace('\\t', ''))\n",
    "#到该案由路径下载入获取已经训练好的模型\n",
    "\n",
    "output = os.path.join (fr,anyou)\n",
    "# 载入字典\n",
    "dictionary = corpora.Dictionary.load(os.path.join(output,\"all.dic\"))\n",
    "# 载入TFIDF模型和索引\n",
    "tfidfModel = models.TfidfModel.load(os.path.join(output,\"allTFIDF.mdl\"))\n",
    "indexTfidf = similarities.MatrixSimilarity.load(os.path.join(output,\"allTFIDF.idx\"))\n",
    "# # 载入LDA模型和索引\n",
    "# ldaModel = models.LdaModel.load(os.path.join(output,  \"allLDA50Topic.mdl\"))\n",
    "# indexLDA = similarities.MatrixSimilarity.load(os.path.join (output,\"allLDA50Topic.idx\"))\n",
    "#query就是测试数据，先切词\n",
    "query_bow = dictionary.doc2bow(text)\n",
    "#使用TFIDF模型向量化\n",
    "tfidfvect = tfidfModel[query_bow]\n",
    "#然后LDA向量化，因为我们训练时的LDA是在TFIDF基础上做的，所以用itidfvect再向量化一次\n",
    "# ldavec = ldaModel[tfidfvect]\n",
    "#TFIDF相似性，相似性列表记录与每份模型中的文书的相似度\n",
    "simstfidf = indexTfidf[tfidfvect]\n",
    "#LDA相似性，相似性列表记录与每份模型中的文书的相似度\n",
    "# simlda = indexLDA[ldavec]\n",
    "# 获取TDIDF模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "num = []\n",
    "# 记录相似度\n",
    "gets = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simstfidf:\n",
    "        if m < likely and likely not in gets:\n",
    "                m = likely\n",
    "                marknow = mark\n",
    "        mark = mark+1\n",
    "    gets.append(m)\n",
    "    num.append(marknow)\n",
    "print(num)\n",
    "bianhao = []\n",
    "for fn in num:\n",
    "    fl = open(os.path.join(fs,'00'+str(fn)+'.txt'),encoding = 'UTF-8')\n",
    "    lines = fl.readlines()\n",
    "    for line in lines:#将文件中的编号装入列表\n",
    "        if len(line) != 1:\n",
    "            if line.split('##')[0] == '编号':\n",
    "                print(line.split('##')[1])\n",
    "                bianhao.append(line.split('##')[1])\n",
    "print(bianhao)\n",
    "\n",
    "# 获取LDA模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "# numlda = []\n",
    "# # 记录相似度\n",
    "# getslda = []\n",
    "# for i in range(0,10):\n",
    "#     # 相似度\n",
    "#     m = 0\n",
    "#     # 文书序号\n",
    "#     mark = 0\n",
    "#     for likely in simlda:\n",
    "#         if m < likely and likely not in getslda:\n",
    "#             m = likely\n",
    "#             marknow = mark\n",
    "#         mark = mark+1\n",
    "#     getslda.append(m)\n",
    "#     numlda.append(marknow)\n",
    "# print(numlda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = os.listdir(r'alltext')\n",
    "len(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10578, 4908, 16473, 1272, 1044, 7996, 3780, 19378, 7995]\n",
      "[0, 136, 8270, 3377, 4739, 4738, 4776, 3543, 1449, 4733]\n"
     ]
    }
   ],
   "source": [
    "# 获取TDIDF模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "num = []\n",
    "# 记录相似度\n",
    "gets = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simstfidf:\n",
    "        if m < likely and likely not in gets:\n",
    "                m = likely\n",
    "                marknow = mark\n",
    "        mark = mark+1\n",
    "    gets.append(m)\n",
    "    num.append(marknow)\n",
    "print(num)\n",
    "\n",
    "# 获取LDA模型中与该文档前十相近的文书号\n",
    "# 记录序号\n",
    "numlda = []\n",
    "# 记录相似度\n",
    "getslda = []\n",
    "for i in range(0,10):\n",
    "    # 相似度\n",
    "    m = 0\n",
    "    # 文书序号\n",
    "    mark = 0\n",
    "    for likely in simlda:\n",
    "        if m < likely and likely not in getslda:\n",
    "            m = likely\n",
    "            marknow = mark\n",
    "        mark = mark+1\n",
    "    getslda.append(m)\n",
    "    numlda.append(marknow)\n",
    "print(numlda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.01392374, 0.01199614, ..., 0.03279732, 0.0635283 ,\n",
       "       0.04038719], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simstfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帮助犯罪分子逃避处罚罪抢劫罪受贿罪\n"
     ]
    }
   ],
   "source": [
    "fr = r'result'\n",
    "print (anyou)\n",
    "output = os.path.join(fr, anyou) #输出文件目录\n",
    "if not os.path.exists(output):    \n",
    "    os.makedirs(output)\n",
    "        # 生成字典\n",
    "dictionary = corpora.Dictionary(train_set)\n",
    "    # 去除极低频的杂质词\n",
    "dictionary.filter_extremes(no_below=1, no_above=1, keep_n=None)\n",
    "    # 将词典保存下来，方便后续使用\n",
    "dictionary.save(os.path.join(output, \"all.dic\"))\n",
    "corpus = [dictionary.doc2bow(text) for text in train_set]\n",
    "    # 使用数字语料生成TFIDF模型\n",
    "tfidfModel = models.TfidfModel(corpus)\n",
    "    # 存储tfidfModel\n",
    "tfidfModel.save(os.path.join(output, \"allTFIDF.mdl\"))\n",
    "    # 把全部语料向量化成TFIDF模式，这个tfidfModel可以传入二维数组\n",
    "    \n",
    "tfidfVectors = tfidfModel[corpus]\n",
    "    # 建立索引并保存\n",
    "indexTfidf = similarities.MatrixSimilarity(tfidfVectors)\n",
    "indexTfidf.save(os.path.join(output,  \"allTFIDF.idx\"))\n",
    "    # 通过TFIDF向量生成LDA模型，id2word表示编号的对应词典，num_topics表示主题数，我们这里设定的50，主题太多时间受不了。\n",
    "lda = models.LdaModel(tfidfVectors, id2word=dictionary, num_topics=100)\n",
    "    # 把模型保存下来\n",
    "lda.save(os.path.join(output, \"allLDA50Topic.mdl\"))\n",
    "    # 把所有TFIDF向量变成LDA的向量\n",
    "corpus_lda = lda[tfidfVectors]\n",
    "    # 建立索引，把LDA数据保存下来\n",
    "indexLDA = similarities.MatrixSimilarity(corpus_lda)\n",
    "indexLDA.save(os.path.join(output,\"allLDA50Topic.idx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 重新编号\n",
    "fa = os.listdir(r'alltext')\n",
    "f = r'alltext'\n",
    "fr = r'result'\n",
    "for f1 in fa:#遍历整个一级文件夹\n",
    "    train_set = []\n",
    "    tmp_pathall = os.path.join(f, f1)\n",
    "    fs = os.listdir(tmp_pathall)\n",
    "    i=0\n",
    "    for f2 in fs:#遍历单个案由的所有文件\n",
    "        j = '00'+str(i)+'.txt'\n",
    "        tmp_path = os.path.join(tmp_pathall,f2)\n",
    "        os.rename(tmp_path, os.path.join(tmp_pathall,j))\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
